## 2. Grundlagen der EU KI-Verordnung


Der Begriff der KI ist zentraler Anknüpfungspunkt der KIV mit dem Begriff des KI-Systems. Gemäß Art. 3 Abs. 1 KIV sei ein KI-System “ein maschinengestütztes System, das so konzipiert ist, dass es mit unterschiedlichem Grad an Autonomie operieren kann und nach dem Einsatz Anpassungsfähigkeit zeigen kann, und das für explizite oder implizite Ziele aus den Eingaben, die es erhält, ableitet [...]”. Hauptmerkmal sei die Fähigkeit, eigene Schlüsse in Bezug auf die Gewinnung von Ergebnissen, wie Vorhersagen, Inhalte, Empfehlungen oder Entscheidungen, zu ziehen (EG 12 KIV). Diese Definition soll gemäß Erwägungsgrund (nachfolgend “EG”) 12 sich an die Definition internationaler Organisationen anlehnen, um Rechtssicherheit, internationale Konvergenz zu gewährleisten und breite Akzeptanz unter Berücksichtigung der notwendigen Flexibilität zu ermöglichen. Wesentlich sei die Unterscheidung von herkömmlichen, regelbasierten Softwaresystemen oder Programmieransätzen. Zudem erfolgt eine Risikokartierung je nach Grad der Gefahr sowie erforderlicher Maßnahmen (CEDPO AI Working Group, 2023).

Trotz des Nutzens für Wirtschaft und Gesellschaft in Hinblick auf konkrete Anwendungsfälle, kann die Nutzung von KI Risiken mit sich bringen und öffentliche Interessen und Rechte schädigen (Frank & Heine 2023; EG 3 und 4 KIV).

Zunächst hat ein Unternehmen deshalb zu prüfen, welcher Risikokartierung ein KI-System nach der KIV zuzuordnen ist. Zu diesem Zweck ist die konkrete beabsichtigte Verwendungsabsicht mit dem bestimmungsmäßigen Gebrauch des Systems entscheidend (Frank & Heine, 2023). Dafür schlägt die EU-Kommission einen vierstufigen, risikobasierten Ansatz vor. Die Einstufung von KI-Systemen korrespondiert dabei mit steigenden Anforderungen und Verpflichtungen.

Aufgrund der unterstellten Verletzung von Grundrechten der EU sind KI-Anwendungen mit unannehmbarem Risiko grundsätzlich nicht erlaubt (Günther et al., 2024). Verbotene KI-Systeme und Praktiken sind Systeme, die gegen Grundrechte verstoßen, ein hohes Potential für die Manipulation oder Täuschung von Personen bergen, Social Scoring betreiben oder ausbeuterische Praktiken nutzen (Art. 5 KIV). Diese KI-Systeme mit einem unannehmbaren Risiko sind ab sechs Monaten nach Inkrafttreten der Verordnung verboten (EG 179 KIV).

Hochrisiko-KI-Systeme sind KI-Anwendungen mit einem hohen Risiko für Gesundheit, Sicherheit oder Grundrechte natürlicher Personen. Anbieter eines Hochrisiko-KI-Systems ist, wer ein Hochrisiko-KI-System mit eigenem Namen oder Handelsmarke versieht, obgleich dies bereits in Verkehr gebracht oder in Betrieb genommen, wesentliche Änderungen vorgenommen werden, Nutzungszwecke so geändert werden, dass das KI-System dann als Hochrisiko-KI-System gilt (Art. 25 Abs. 1 lit a,b KIV). Im EU-Binnenmarkt genutzte Hochrisiko-KI-Systeme müssen spezifische Anforderungen erfüllen und sich einem Konformitätsbewertungsverfahren unterziehen (Art. 6 - 15, 40 - 45 KIV).

Als KI-Systeme mit begrenztem Risiko gelten Systeme, welche die “Entscheidungsfindung nicht wesentlich beeinflussen oder diese Interessen nicht wesentlich beeinträchtigen” (EG 53 KIV). Zu diesen Systemen zählen beispielsweise Programme, die unstrukturierte Daten aufbereiten oder Dokumente kategorisieren. Auch Systeme mit allgemeinem Verwendungszweck wie solche, die auf Generative Pre-Trained Transformers (nachfolgend “GPT”) beruhen, fallen in diese Risikoklasse. Für sie gelten vor allem Transparenzpflichten nach Artikel 50 und 51.

Anwendungen, von denen nur ein minimales Risiko für die Sicherheit der Bürger ausgeht, bleiben unreguliert. Spam-Filter oder KI-gestützte Computerspiele können hier als Beispiel angeführt werden (Dahm & Twesten, 2023, S. 18).

Im Folgenden werden die Unternehmenspflichten für KI-Systeme mit hohem und begrenztem Risiko näher erläutert.